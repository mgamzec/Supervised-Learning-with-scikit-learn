{"cells":[{"source":"# Supervised Learning with scikit-learn\n","metadata":{"id":"bA5ajAmk7XH6"},"id":"prostate-arizona","cell_type":"markdown"},{"source":"# Importing pandas\nimport pandas as pd\n\n# Importing the course datasets \ndiabetes = pd.read_csv('datasets/diabetes_clean.csv')\nmusic = pd.read_csv('datasets/music_clean.csv')\nadvertising = pd.read_csv('datasets/advertising_and_sales_clean.csv')\ntelecom = pd.read_csv(\"datasets/telecom_churn_clean.csv\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionTime":54,"lastSuccessfullyExecutedCode":"# Importing pandas\nimport pandas as pd\n\n# Importing the course datasets \ndiabetes = pd.read_csv('datasets/diabetes_clean.csv')\nmusic = pd.read_csv('datasets/music_clean.csv')\nadvertising = pd.read_csv('datasets/advertising_and_sales_clean.csv')\ntelecom = pd.read_csv(\"datasets/telecom_churn_clean.csv\")"},"id":"2e25fdd8-4d84-45bc-80f0-949917e00a17","cell_type":"code","execution_count":1,"outputs":[]},{"source":"## k-Nearest Neighbors: Fit","metadata":{},"cell_type":"markdown","id":"d3deff2c-4c3a-40ab-b3db-d2429b926618"},{"source":"# Import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsClassifier \n\n# Create arrays for the features and the target variable\ny = churn_df[\"churn\"].values\nX = churn_df[[\"account_length\", \"customer_service_calls\"]].values\n\n# Create a KNN classifier with 6 neighbors\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the data\nknn.fit(X, y)","metadata":{},"cell_type":"code","id":"6f694831-f659-4f47-a28c-8e7721c4508d","execution_count":null,"outputs":[]},{"source":"## k-Nearest Neighbors: Predict\nNow you have fit a KNN classifier, you can use it to predict the label of new data points.","metadata":{},"cell_type":"markdown","id":"9237869c-f7a9-4e28-a64f-9b450a895756"},{"source":"X_new = np.array([[30.0, 17.5],\n                  [107.0, 24.1],\n                  [213.0, 10.9]])","metadata":{},"cell_type":"code","id":"04eeeaf2-6acf-4e5e-9668-71dc00de1d6d","execution_count":null,"outputs":[]},{"source":"# Predict the labels for the X_new\ny_pred = knn.predict(X_new)\n\n# Print the predictions for X_new\nprint(\"Predictions: {}\".format(y_pred)) ","metadata":{},"cell_type":"code","id":"d66c8275-0c01-4c19-8282-ad3537a6ccb8","execution_count":null,"outputs":[]},{"source":"<script.py> output:\n    Predictions: [0 1 0]","metadata":{},"cell_type":"code","id":"b6c950b1-2d73-4eca-95a2-5406251fd93b","execution_count":null,"outputs":[]},{"source":"## Train/test split + computing accuracy","metadata":{},"cell_type":"markdown","id":"5eab1a4d-6c32-455c-a051-d8491d6ee45e"},{"source":"# Import the module\nfrom sklearn.model_selection import train_test_split\n\nX = churn_df.drop(\"churn\", axis=1).values\ny = churn_df[\"churn\"].values\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Print the accuracy\nprint(knn.score(X_test, y_test))","metadata":{},"cell_type":"code","id":"bb216fb3-1c1a-4314-9680-fc960032e8db","execution_count":null,"outputs":[]},{"source":"<script.py> output:\n    Predictions: [0 1 0]\n\n<script.py> output:\n    0.8740629685157422","metadata":{},"cell_type":"code","id":"0e9f8dfe-db1f-47f8-95c8-527706d7aa1a","execution_count":null,"outputs":[]},{"source":"## Overfitting and underfitting\nInterpreting model complexity is a great way to evaluate performance when utilizing supervised learning.Aim is to produce a model that can interpret the relationship between features and the target variable, as well as generalize well when exposed to new observations.\n\n### Instructions\n\nCreate neighbors as a numpy array of values from 1 up to and including 12.\nInstantiate a KNN classifier, with the number of neighbors equal to the neighbor iterator.\nFit the model to the training data.\nCalculate accuracy scores for the training set and test set separately using the .score() method, and assign the results to the index of the train_accuracies and test_accuracies dictionaries, respectively.","metadata":{},"cell_type":"markdown","id":"5ae0b8d3-422d-42aa-a7da-950f33f14e38"},{"source":"# Create neighbors\nneighbors = np.arange(1, 13)\ntrain_accuracies = {}\ntest_accuracies = {}\n\nfor neighbor in neighbors:\n  \n  \t# Set up a KNN Classifier\n  \tknn = KNeighborsClassifier(n_neighbors=neighbor)\n  \n  \t# Fit the model\n  \tknn.fit(X_train, y_train)\n  \n  \t# Compute accuracy\n  \ttrain_accuracies[neighbor] = knn.score(X_train, y_train)\n  \ttest_accuracies[neighbor] = knn.score(X_test, y_test)\nprint(neighbors, '\\n', train_accuracies, '\\n', test_accuracies)","metadata":{},"cell_type":"code","id":"5e06198d-2db2-42fc-b8b4-e17f158ac1c9","execution_count":null,"outputs":[]},{"source":"<script.py> output:\n    Predictions: [0 1 0]\n\n<script.py> output:\n    0.8740629685157422\n\n<script.py> output:\n    [ 1  2  3  4  5  6  7  8  9 10 11 12] \n     {1: 1.0, 2: 0.887943971985993, 3: 0.9069534767383692, 4: 0.8734367183591796, 5: 0.8829414707353677, 6: 0.8689344672336168, 7: 0.8754377188594297, 8: 0.8659329664832416, 9: 0.8679339669834918, 10: 0.8629314657328664, 11: 0.864432216108054, 12: 0.8604302151075538} \n     {1: 0.7871064467766117, 2: 0.8500749625187406, 3: 0.8425787106446777, 4: 0.856071964017991, 5: 0.8553223388305847, 6: 0.861319340329835, 7: 0.863568215892054, 8: 0.8605697151424287, 9: 0.8620689655172413, 10: 0.8598200899550225, 11: 0.8598200899550225, 12: 0.8590704647676162}","metadata":{},"cell_type":"code","id":"3e46e057-175a-49e1-bf65-359b39f3a400","execution_count":null,"outputs":[]},{"source":"## Visualizing model complexity\nNow you have calculated the accuracy of the KNN model on the training and test sets using various values of n_neighbors, you can create a model complexity curve to visualize how performance changes as the model becomes less complex.\n\n### Instructions\n\n- Add a title \"KNN: Varying Number of Neighbors\".\n- Plot the .values() method of train_accuracies on the y-axis against neighbors on the x-axis, with a label of \"Training Accuracy\".\n- Plot the .values() method of test_accuracies on the y-axis against neighbors on the x-axis, with a label of \"Testing Accuracy\".\n- Display the plot.","metadata":{},"cell_type":"markdown","id":"90d1df0d-cd4a-4c36-8bad-9ea6663a117b"},{"source":"# Add a title\nplt.title(\"KNN: Varying Number of Neighbors\")\n\n# Plot training accuracies\nplt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n\n# Plot test accuracies\nplt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n\nplt.legend()\nplt.xlabel(\"Number of Neighbors\")\nplt.ylabel(\"Accuracy\")\n\n# Display the plot\nplt.show()","metadata":{},"cell_type":"code","id":"f76080b4-4ea7-45df-ad8b-8ad352909352","execution_count":null,"outputs":[]},{"source":"![resim_2023-03-25_155416656](resim_2023-03-25_155416656.png)\n","metadata":{},"cell_type":"markdown","id":"53a2206f-0cce-403a-835f-8963896cbf49"},{"source":"![resim_2023-03-25_155219183](resim_2023-03-25_155219183.png)\n","metadata":{},"cell_type":"markdown","id":"f4374909-cf2b-4a4e-8f79-eedcd5e72ee1"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}